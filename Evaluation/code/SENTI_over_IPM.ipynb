import re
from pathlib import Path
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import pandas as pd
from pathlib import Path
# --------------------------- USER CONFIG -----------------------------
ROOT   = Path("/root/workspace/evaluation/DDI_results")
OUTDIR = ROOT / "relative_improvement"
OUTDIR.mkdir(exist_ok=True)

DATASETS = ["adultsample", "australian", "contraceptive", "credit", "imdb"]
SEEDS    = [94, 584, 1234]

PATTERNS = {
    "sent":    "SENT-I_evaluations_{dat}_{seed}.csv",
    "fixed":   "IPM_evaluations_fixed_{dat}_{seed}.csv",
    "retrain": "IPM_evaluations_Retraining_{dat}_{seed}.csv",}
    
# ------------------------- DETAILED CSV GENERATION -------------------------
all_results = []
null_col_name = None
for dat in DATASETS:
    for seed in SEEDS:
        # Build file paths for the three evaluation types
        path_sent    = ROOT / PATTERNS["sent"].format(dat=dat, seed=seed)
        path_fixed   = ROOT / PATTERNS["fixed"].format(dat=dat, seed=seed)
        path_retrain = ROOT / PATTERNS["retrain"].format(dat=dat, seed=seed)

        # Skip if any file is missing
        if not (path_sent.exists() and path_fixed.exists() and path_retrain.exists()):
            print(f"[WARN] Missing raw CSV for {dat}, seed {seed}")
            continue

        # Load DataFrames
        df_sent    = pd.read_csv(path_sent)
        df_fixed   = pd.read_csv(path_fixed)
        df_retrain = pd.read_csv(path_retrain)

        # Detect and validate null-fraction column in all three
        cols_sent    = [c for c in df_sent.columns if "nulls" in c.lower()]
        cols_fixed   = [c for c in df_fixed.columns if "nulls" in c.lower()]
        cols_retrain = [c for c in df_retrain.columns if "nulls" in c.lower()]
        if not cols_sent or not cols_fixed or not cols_retrain:
            print(f"[ERROR] Null column missing for {dat}, seed {seed}")
            continue
        if len({cols_sent[0], cols_fixed[0], cols_retrain[0]}) != 1:
            print(f"[ERROR] Inconsistent null columns for {dat}, seed {seed}")
            continue
        null_col = cols_sent[0]
        if null_col_name is None:
            null_col_name = null_col

        # Define metric columns
        sim_s_col, time_s_col = "avg_semantic_sim_SENTI", "total_time_SENTI"
        sim_f_col, time_f_col = "avg_semantic_sim_IPM_70_30_fixed", "total_time_IPM_70_30_fixed"
        sim_r_col, time_r_col = "avg_semantic_sim_IPM_70_30_Retraining", "total_time_IPM_70_30_Retraining"

        # Select and merge on indices + null fraction
        df1 = df_sent[[null_col, "start_index", "end_index", sim_s_col, time_s_col]]
        df2 = df_fixed[[null_col, "start_index", "end_index", sim_f_col, time_f_col]]
        df3 = df_retrain[[null_col, "start_index", "end_index", sim_r_col, time_r_col]]
        df = df1.merge(df2, on=[null_col, "start_index", "end_index"]) \
               .merge(df3, on=[null_col, "start_index", "end_index"])

        # Compute relative improvements in semantic similarity
        df["Over_IPM_fixed"]     = (df[sim_s_col] - df[sim_f_col]) / df[sim_f_col] * 100
        df["Over_IPM_retrained"] = (df[sim_s_col] - df[sim_r_col]) / df[sim_r_col] * 100

        # Compute time savings percentages
        df["OverTime_IPM_fixed"]     = (df[time_f_col] - df[time_s_col]) / df[time_f_col] * 100
        df["OverTime_IPM_retrained"] = (df[time_r_col] - df[time_s_col]) / df[time_r_col] * 100

        # Compute runtime fractions
        df["FracTime_IPM_fixed"]     = df[time_f_col] / df[time_s_col]
        df["FracTime_IPM_retrained"] = df[time_r_col] / df[time_s_col]

        # Add context columns
        df.insert(0, "seed",  seed)
        df.insert(0, "dataset", dat)

        # Save detailed per-seed CSV
        out_path = OUTDIR / f"{dat}_{seed}_relative_improvement.csv"
        df.to_csv(out_path, index=False)
        print(f"[INFO] Saved detailed improvements: {out_path}")

        all_results.append((dat, df))

# ------------------------- CONCATENATE PER-DATASET -------------------------
if all_results and null_col_name:
    # Group results by dataset
    from collections import defaultdict
    grouped = defaultdict(list)
    for dat, df in all_results:
        grouped[dat].append(df)

    # For each dataset, concat seed-level details and save
    concat_results = {}
    for dat, dfs in grouped.items():
        df_concat = pd.concat(dfs, ignore_index=True)
        concat_path = OUTDIR / f"{dat}_relative_improvement.csv"
        df_concat.to_csv(concat_path, index=False)
        print(f"[INFO] Saved concatenated file for {dat}: {concat_path}")
        concat_results[dat] = df_concat

    # ------------------------- AGGREGATE ACROSS SEEDS -------------------------
    for dat, df_concat in concat_results.items():
        summary = df_concat.groupby(["dataset", null_col_name]).agg({
            "Over_IPM_fixed":           "mean",
            "Over_IPM_retrained":       "mean",
            "OverTime_IPM_fixed":       "mean",
            "OverTime_IPM_retrained":   "mean",
            "FracTime_IPM_fixed":       "mean",
            "FracTime_IPM_retrained":   "mean",
        }).reset_index().rename(columns={
            null_col_name:                "pct_nulls",
            "Over_IPM_fixed":            "avg_over_sim_IPM_fixed(%)",
            "Over_IPM_retrained":        "avg_over_sim_IPM_retrained(%)",
            "OverTime_IPM_fixed":        "avg_over_time_IPM_fixed(%)",
            "OverTime_IPM_retrained":    "avg_over_time_IPM_retrained(%)",
            "FracTime_IPM_fixed":        "avg_frac_time_IPM_fixed",
            "FracTime_IPM_retrained":    "avg_frac_time_IPM_retrained",
        }).sort_values(["pct_nulls"])

        summary_path = OUTDIR / f"average_relative_improvement_{dat}.csv"
        summary.to_csv(summary_path, index=False)
        print(f"[INFO] Saved averaged summary for dataset '{dat}': {summary_path}")
else:
    print("[INFO] No data to summarize or null column not detected.")

