## Requirements

- torch>=1.1.0
- pandas>=0.25.3
- numpy>=1.19.5
- pretrained language models can be found in [Hugging Face](https://huggingface.co/).

## Usage

Before running the `ipm_multi_fixed.py` or  `ipm_multi_Retraining.py`, the user need to transfer all the nonimputed files into the IPM data directory.


### Important Arguments

- `--data_dir` : Directory of data files. 
- `--dataset` : Name of the dataset.
- `--model_name_or_path` : Pretrained language model name or path.
- `--model_type` : Type of the pretrained model. Default is roberta.
- `--do_lower_case` : Whether tansform inputs into lower case letters. Default True.
- `--max_seq_length` : Maximum length of the input sequence.
- `--train_batch_size` : Batch size for training.
- `--eval_batch_size` : Batch size for evaluation.
- `--num_epochs` : Number of epochs for training.
- `--neg_num` : Number of negative examples. Default is 3.